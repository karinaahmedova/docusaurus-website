(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[645],{3905:function(e,t,n){"use strict";n.d(t,{Zo:function(){return u},kt:function(){return d}});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),p=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},u=function(e){var t=p(e.components);return r.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),m=p(n),d=a,_=m["".concat(l,".").concat(d)]||m[d]||c[d]||i;return n?r.createElement(_,o(o({ref:t},u),{},{components:n})):r.createElement(_,o({ref:t},u))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,o=new Array(i);o[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:a,o[1]=s;for(var p=2;p<i;p++)o[p]=n[p];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}m.displayName="MDXCreateElement"},4804:function(e,t,n){"use strict";n.r(t),n.d(t,{frontMatter:function(){return s},contentTitle:function(){return l},metadata:function(){return p},toc:function(){return u},default:function(){return m}});var r=n(2122),a=n(9756),i=(n(7294),n(3905)),o=["components"],s={sidebar_position:1},l="Dagster",p={unversionedId:"tools/dagster",id:"tools/dagster",isDocsHomePage:!1,title:"Dagster",description:"I was thinking where I should locate dagster in my library? python? ML? ETL?",source:"@site/docs/tools/dagster.md",sourceDirName:"tools",slug:"/tools/dagster",permalink:"/my-website/docs/tools/dagster",editUrl:"https://github.com/karinaahmedova/docusaurus-website/master/website/docs/tools/dagster.md",version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Azure ML",permalink:"/my-website/docs/ml/azureml"}},u=[{value:"Solids and Pipelines",id:"solids-and-pipelines",children:[{value:"Single Solid Pipeline",id:"single-solid-pipeline",children:[]},{value:"Multiple Solids into Pipeline",id:"multiple-solids-into-pipeline",children:[]}]},{value:"Testing Solids and Pipelines",id:"testing-solids-and-pipelines",children:[]}],c={toc:u};function m(e){var t=e.components,n=(0,a.Z)(e,o);return(0,i.kt)("wrapper",(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"dagster"},"Dagster"),(0,i.kt)("p",null,"I was thinking where I should locate dagster in my library? python? ML? ETL?"),(0,i.kt)("p",null,"I think it's should been as a separate data orchestration tool for machine learning, analytics, and ETL."),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://dagster.io/"},"https://dagster.io/")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"pip install dagster dagit\n\n")),(0,i.kt)("h2",{id:"solids-and-pipelines"},"Solids and Pipelines"),(0,i.kt)("h3",{id:"single-solid-pipeline"},"Single Solid Pipeline"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Solids")," are individual units of computation that wire together to form ",(0,i.kt)("strong",{parentName:"p"},"pipeline")),(0,i.kt)("p",null,"Lets make a first example where we would like to load dataframe"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'from dagster import solid\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.dataframe import DataFrame\n\n@solid\ndef get_customer_list(_) -> DataFrame:\n    spark = SparkSession.builder.getOrCreate()\n    return (\n        spark.read\n        .option("mergeSchema", True)\n        .orc("/datalake/test/customer")\n        .select(\n            "id"\n            ,"date_part"\n            ,"name"\n            ,"surname"\n            ,"lastmodified"\n        )\n    )\n\n')),(0,i.kt)("p",null,"and to execute ",(0,i.kt)("strong",{parentName:"p"},"@solid")," we use ",(0,i.kt)("strong",{parentName:"p"},"@pipeline")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'from dagster import pipeline, execute_pipeline\n\nfrom datasets.customers import get_customer_list\n\n@pipeline\ndef nightly():\n    customer_list = get_customer_list()\n\nif __name__ == "__main__":\n    execute_pipeline(nightly, run_config={})\n\n')),(0,i.kt)("h3",{id:"multiple-solids-into-pipeline"},"Multiple Solids into Pipeline"),(0,i.kt)("p",null,"Now we look into our Single Solid Pipeline `",(0,i.kt)("inlineCode",{parentName:"p"},"get_customer_list")," and add two more solids which will create a programm to merge into current customer list only updated customers without running all partitions in the datalake."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'from datetime import datetime\nfrom dagster import solid, Output, OutputDefinition\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.dataframe import DataFrame\nfrom pyspark.sql.types import StructType, StructField, StringType, TimestampType\n\nPATH = "/datalake/test/customer"\n\n@solid\ndef get_current_customer(_) -> DataFrame:\n    spark = SparkSession.builder.getOrCreate()\n    try:\n        df = spark.read.format("delta").load(PATH)\n        if len(df.columns) == 0:\n            raise Exception("Empty DataFrame")\n    except:\n        # For the first time create the key and date columns\n        schema = StructType([\n            StructField("id", StringType(), True),\n            StructField("date_part", StringType(), True),\n            StructField("name", StringType(), True),\n            StructField("surname", StringType(), True),\n            StructField("lastmodified", StringType(), True),\n        ])\n        spark.createDataFrame(spark.sparkContext.emptyRDD(), schema).write.format("delta").mode("overwrite").save(PATH)\n        df = spark.read.format("delta").load(PATH)\n\n    return df\n\n@solid\ndef merge_new_customer_into_current(_, source: DataFrame, current: DataFrame) -> DataFrame:\n    spark = SparkSession.builder.getOrCreate()\n    now = datetime.utcnow()\n\n    timestamp_column = "lastmodified"\n    key_column = "id"\n\n    source.createOrReplaceTempView("source")\n    current.createOrReplaceTempView("current")\n    incremental = spark.sql(f"""\n        WITH latest_date_part_in_current AS (\n            SELECT COALESCE(max(date_part), "1970-01-01") FROM current\n        ),\n        lastest_record_in_current AS (\n            SELECT COALESCE(max({timestamp_column}), "1970-01-01T00:00:00.000") FROM current\n            WHERE date_part >= (SELECT * FROM latest_date_part_in_current)\n        ),\n        new_records_in_source AS (\n            SELECT * FROM source\n            WHERE ({key_column} IS NOT NULL) AND (date_part >= (SELECT * FROM latest_date_part_in_current)) AND ({timestamp_column} > (SELECT * FROM lastest_record_in_current))\n        ),\n        updates AS (\n            SELECT\n                *,\n                ROW_NUMBER() OVER (PARTITION BY {key_column} ORDER BY {timestamp_column} DESC) AS row\n            FROM new_records_in_source\n        )\n        SELECT *\n        FROM updates\n        WHERE row = 1\n    """)\n    incremental = incremental.persist()\n    incremental.createOrReplaceTempView("incremental")\n\n    spark.sql(f"""\n        SELECT MIN(date_part), MAX(date_part), MIN({timestamp_column}), MAX({timestamp_column}), COUNT(*) FROM incremental\n    """).show(1, False)\n\n    spark.sql(f"""\n        MERGE INTO delta.`{PATH}` current\n        USING incremental\n        ON current.{key_column} = incremental.{key_column}\n        WHEN MATCHED THEN UPDATE SET *\n        WHEN NOT MATCHED THEN INSERT *\n    """)\n\n    incremental.unpersist()\n    spark.catalog.dropTempView("incremental")\n    spark.catalog.dropTempView("source")\n    spark.catalog.dropTempView("current")\n\n    spark.sql(f"""\n        VACUUM delta.`{PATH}`\n    """)\n\n    return current\n\n')),(0,i.kt)("p",null,"and run ",(0,i.kt)("strong",{parentName:"p"},"@solids")," in ",(0,i.kt)("strong",{parentName:"p"},"@pipeline")," -  one nightly job:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'from dagster import pipeline, execute_pipeline\n\nfrom datasets.customers import get_customer_list\nfrom datasets.customers_current import get_current_customer, merge_new_customer_into_current\n\n@pipeline\ndef nightly():\n    customer_list = get_customer_list()\n    customer_current = get_current_customer()\n    merge_new_customer_into_current(customer_list, customer_current)\n\nif __name__ == "__main__":\n    execute_pipeline(nightly, run_config={})\n\n\n')),(0,i.kt)("h2",{id:"testing-solids-and-pipelines"},"Testing Solids and Pipelines"),(0,i.kt)("p",null,"We'll use execute_pipeline() to test our pipeline, as well as execute_solid() to test our solid in isolation."))}m.isMDXComponent=!0}}]);